
\documentclass[ 11pt, a4paper, parskip=half*, captions=tableheading]{scrartcl}

%\documentclass[
%10pt,twoside,smallheadings,leqno,pointlessnumbers,BCOR8mm,DIV11]
%parskip=half für Absatzabstand, statt Einzug
%{scrreprt}

%lenqno setzt gleichungsnummern links
%BCOR8.25mm, Rand, der durch bindung verdeckt wird %DIV10 Verälthnis der Raender, DIVcalc berechnet DIV

\usepackage[T1]{fontenc}		% richtige Zeichenkodierung
\usepackage{lmodern}			% sonst gibts mit \usepackage[T1]{fontenc} bitmap-schriften

\usepackage[USenglish]{babel}	%englische silbentrennung
\usepackage{graphicx} 			%zum graphiken einbinden
\usepackage{amssymb}			%AMS mathe szmbole
\usepackage{amsmath}
\usepackage{color}
\usepackage{microtype}		%Schöneren Satz 
\usepackage{textcomp}			%für Celsiussymbol
\usepackage{tabularx}			%fuer Tabellenumgebung
\usepackage{pdfpages}			%kann mit \includepdf[pages={-}]{...pdf} pdfs einbinden
\usepackage{float}                          %damit Bild an dem Ort, wo es sein soll
%[H] genau hier,  [h] etwa hier, [t] zu beginn der seite, [p] auf eigener seite

%\usepackage[ansinew]{inputenc}   %für Umlaute (Windows)
\usepackage[applemac]{inputenc} %für Umlaute (Mac)

%\usepackage{setspace}			%Zeilenabstand ändern: %Zeilenabstand ändern \begin{onehalfspacing} \begin{singlespacing}
%\usepackage{enumitem} 			%um Aufzählungen anzupassen

\usepackage{listings}			%Code mit syntax higlighting einfügen
\lstset{						%Code darstellung (fure R)
		backgroundcolor=\color[rgb]{0.90,0.90,0.91},
		language=R,
		basicstyle=\scriptsize,
		breaklines=true,
		keywordstyle=\color[rgb]{0,0,1},
		commentstyle=\color[rgb]{0,0.5,0},
		stringstyle=\color[rgb]{0.48,0.42,0.46},
}


\definecolor{darkblue}{rgb}{0,0,0.5}
\usepackage[colorlinks=true,linkcolor=darkblue,urlcolor=darkblue]{hyperref}	%fuer pdf-links

%Beschriftung von Abbildungen (gibt wahrscheinlich elegantere Lösung mit KOMA
\usepackage[hang,small,bf]{caption}
%bis auf welcher Gliederungsebene Nummerieren
\setcounter{secnumdepth}{3} 
%welche Ebenen im Inhaltsverzeichniss
\setcounter{tocdepth}{1}


\usepackage{ellipsis}			%Korrekte Abstände bei Auslassunsgzeichen (\dots)

%=========================================================
% Titel

\title{Guide Line for the Application of Approximative Bayesian Computing Algorithms}
\subtitle{Sampling of the Posterior Distribution without Likelihoods} 
\date{\today}
\author{\href{mailto: scheidegger.a@gmail.com}{Andreas Scheidegger}}


\begin{document}

\maketitle

%=========================================================
% Abstakt

\begin{abstract}

It is often impossible to derive the likelihood function of complex stochastic models.
Nevertheless, the posteriori distribution of the parameter can be computed with approximative Bayesian computing algorithms---without using a likelihood function. However, there are some pitfalls in the practical application. This guide line provides a generic setup for the use of efficient approximative Bayesian computing methods.
\end{abstract}

%=========================================================
% Inhalt
\tableofcontents

%=========================================================
% Intro
\section{Introduction}

The classical Bayesian methods to compute the posterior distribution of the parameters are difficult to apply for stochastic models. The traditional methods require a likelihood function which is often impossible to derive for stochastic models.  However, approximative Bayesian computing (ABC) methods allows sampling from the---at least approximative---posterior distribution without using a likelihood function.

Although some ABC methods are quite simpel, there are several pitfalls in the application. Efficient sampling algorithms like Markov Chain Monte Carlo (MCMC) or Partial Rejection Control (PRC) adapted for ABC require five things:
\begin{enumerate}
\item A stochastic model $M(\theta)$ which can simulate data given the parameter vector $\theta$ 
\item A set of summary statistics $S_i$ which \emph{sufficiently} described the (simulated or real) data  
\item A distance function $\rho(\cdot, \cdot)$ which quantifies the (dis)similarity of data
\item A tolerance $\epsilon$ which is the maximum distance to accept a proposed parameter vector
\item A transition kernel $q(\cdot)$ 
\end{enumerate}
Especially the points 2 and 4 are ticklish. The choice of the summary statistics is important, because they should reflect all characteristics of the data. A summary statistic is \emph{sufficient} ``when no other statistic which can be calculated from the same sample provides additional information as to the value of the parameter [of the distribution]'' (Fisher, 1922; see also Wikipedia, 2010). 
%---------------------------------
\subsection*{A Preliminaries}
\subsubsection*{A1 Presampling}
Samples of a presampling are necessary to find the right summary statistics and for choosing the tolerance. 
\begin{enumerate}
\item sample $N_{pre}$ parameter vectors $\theta_n,\,n=1,\dots,N_{pre}$ from the prior density $\pi(\theta)$
\item simulate data $D_n^{sim}$ with model $M(\theta_n)$ for each parameter vector $\theta_n,\,n=1,\dots,N_{pre}$
\item compute all $I$ summary statistics for each $D_n^{sim}$
\end{enumerate}

%----------------------
\subsection*{B Sampling}
After the preliminaries, the rest of the sampling can be done with more efficient sampling methods. Possible algorithms are Markov chain Monte Carlo without likelihoods and Partial Rejection Control without likelihoods (see below). 
9). 

%=========================================================
% Literatur
\section{Literature}

{\bf Fisher, M.A. (1922)}. On the Mathematical Foundations of Theoretical Statistics. \emph{Philosophical Transactions of the Royal Society}. A: 222: 309--368. \url{http://digital.library.adelaide.edu.au/dspace/bitstream/2440/15172/2/18pt1.pdf}, accessed January~28,~2010.

{\bf Gilks, W., Richardson, S. \& Spiegelhalter, D. (1998)}. \emph{Markov Chain Monte Carlo in Practice: Interdisciplinary Statistics}. 2nd Edition. Chapman \& Hall.

%=========================================================
% Anhang
\newpage
\appendix


%=========================================================
% Misc
\section{Miscellaneous}
\label{misc}

\subsection{Variance of the sample mean}
Variance of the sample mean:
\begin{equation}
Var(\hat{\mu}) = \frac{\sigma^2}{n}
\end{equation}
$\hat{\mu}$: the sample mean \\
$\sigma$: standard deviation \\
$n$: number of \emph{independed} samples


\subsection{Variance of the $p$-th sample quantile}
Approximative variance of the $p$-th sample quantile:

\begin{equation}
Var(\hat{x}_p ) = \frac{p(1-p)}{n \cdot f(\hat{x}_p)^2}
\end{equation}
$\hat{x}_p$: the $p$-th quantile of x \\
$f()$: density function
$n$: number of \emph{independed} samples


%=========================================================
% R-code
\section{R-code}
\label{code}

\subsection{Generic Markov Chain Monte Carlo sampler without likelihoods}
%---------------------
\begin{lstlisting}[language=R]
# ===============================================================================
# Approximative Bayesian Computing (ABC)
# Generic Markov Chain Monte Carlo sampler without likelihoods
#
# Andreas Scheidegger
# 10.11.2009
# ===============================================================================

# See Marjoram et al. (2003), Markov Chain Monte Carlo without likelihoods, PNAS 100(26), pp 15324-15328.
# Implementation of algorithm F

# -- Arguments --
# f.dist: function which simualtes data and returns the distance between the real and the simulated data.
#         The first argument must be the parameter vector.
# d.priori: function which returns the density of the prior distribution of a parameter vector.
# n.sample: number of samples
# eps: accepted tolerance between data and model output
# init: initial values
# sigma: vector of standard dev. of the gaussian proposal distribution
# verbose: number of samples between printed outputs
# ...: arguments for f.dist
#
# -- Value --
# matrix containing in each row a (autocorrelated) sample of all parameters and the corresponding delta value

ABC.MCMC <- function (f.dist, d.priori, n.sample, eps, init, sigma, verbose=100, ...) {

  # Number of parameter
  n.para <- length(init)

  # Matrix to store samples
  sample <- matrix(NA, ncol=n.para, nrow=n.sample)
  
  # Vector to store  distances
  delta <- rep(NA, n.sample)

  # Initional values
  sample[1,] <- as.matrix(init)
  delta[1] <- f.dist(sample[1,], ...)
  
  # repeat for each sample
  for (k in 2:n.sample)  {
  
    # print
    if((k %% verbose)==0) cat("k =", k, "\n")

    # F1: generate candidate point
    x.prob <- sample[k-1,]+rnorm(n.para,sd=sigma)    # gaussian proposal density (rounded to integer)
    
    # F2, F3: simulate data and calculate distance to the real data
    dist <- f.dist(x.prob, ...)

    # if dist <= eps go to F4
    if(dist <= eps) {
    
      # F4: calculate prob. h (due to the symmetrie of the proposal density, it is not necessary in F4)
      h <- min(1, d.priori(x.prob)/d.priori(sample[k-1,]) )
      
      # F5: accept x.prob mit prob. h
      if(runif(1)<h) {
        sample[k,] <- x.prob
        delta[k] <- dist
      # else stay on the same point
      } else {
        sample[k,] <- sample[k-1,]
        delta[k] <- delta[k-1]
      }
      
    # else stay on the same point
    } else {
      sample[k,] <- sample[k-1,]
      delta[k] <- delta[k-1]
    }
    
  }
  
  # combine samples and delta
  sample <- cbind(delta, sample)
  if(length(names(init))!=n.para) colnames(sample) <- c("delta", paste("para.",1:n.para, sep="") )
  if(length(names(init))==n.para) colnames(sample) <- c("delta", names(init) )
  
  #return samples and distances
  return(sample)
}
\end{lstlisting}
%---------------

%--------------------------------------
\end{document}
%--------------------------------------
